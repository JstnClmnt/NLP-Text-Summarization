{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Text-Summarization",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JstnClmnt/NLP-Text-Summarization/blob/master/NLP_Text_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruSXNIBlSo6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import numpy as np\n",
        "import re\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "275RJ-74TJUi",
        "colab_type": "code",
        "outputId": "79825a7f-8d46-4baf-e79d-6c40a509581c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJB2GnwJUZJU",
        "colab_type": "code",
        "outputId": "8f0080bc-d2e7-41b1-b324-26f286a2726d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"/content/drive/Team Drives/AI Lords/NLP-Text-Summarization/data/news_summary_more.csv\",encoding=\"latin-1\")\n",
        "df.head()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headlines</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
              "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
              "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
              "      <td>New Zealand defeated India by 8 wickets in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
              "      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
              "      <td>Speaking about the sexual harassment allegatio...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           headlines                                               text\n",
              "0  upGrad learner switches to career in ML & Al w...  Saurav Kant, an alumnus of upGrad and IIIT-B's...\n",
              "1  Delhi techie wins free food from Swiggy for on...  Kunal Shah's credit card bill payment platform...\n",
              "2  New Zealand end Rohit Sharma-led India's 12-ma...  New Zealand defeated India by 8 wickets in the...\n",
              "3  Aegon life iTerm insurance plan helps customer...  With Aegon Life iTerm Insurance plan, customer...\n",
              "4  Have known Hirani for yrs, what if MeToo claim...  Speaking about the sexual harassment allegatio..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUBzG4lWrCW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['text'] = df['text'].str.replace(\"[^a-zA-Z#]\",\" \")\n",
        "df['headlines'] = df['headlines'].str.replace(\"[^a-zA-Z#]\",\" \")\n",
        "df=df.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdIkHeY5pbOW",
        "colab_type": "code",
        "outputId": "58a1623c-e6f2-4da7-c809-50795f038248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(df[\"text\"], df[\"headlines\"], test_size=0.20, random_state=42)\n",
        "\n",
        "print(X_train[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saurav Kant  an alumnus of upGrad and IIIT B s PG Program in Machine learning and Artificial Intelligence  was a Sr Systems Engineer at Infosys with almost   years of work experience  The program and upGrad s     degree career support helped him transition to a Data Scientist at Tech Mahindra with     salary hike  upGrad s Online Power Learning has powered   lakh  careers \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxBxffJa5Q3H",
        "colab_type": "code",
        "outputId": "f6687aa3-68c0-41ec-c219-a687ff85cc9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "X_train_tokenized=[word_tokenize(i)  for i in X_train]\n",
        "X_test_tokenized=[word_tokenize(i) for i in X_test]\n",
        "print(X_train_tokenized[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['Hollywood', 'actress', 'Kate', 'Winslet', 'has', 'joined', 'the', 'cast', 'of', 'Titanic', 'director', 'James', 'Cameron', 's', 'upcoming', 'films', 'in', 'the', 'Avatar', 'franchise', 'making', 'this', 'their', 'first', 'venture', 'together', 'after', 'years', 'of', 'Titanic', 's', 'release', 'Cameron', 'said', 'Kate', 'and', 'I', 'have', 'been', 'looking', 'for', 'something', 'to', 'do', 'together', 'since', 'our', 'collaboration', 'on', 'Titanic', 'I', 'can', 't', 'wait', 'to', 'see', 'her', 'bring', 'the', 'character', 'of', 'Ronal', 'to', 'life']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZfUX-LmsONj",
        "colab_type": "code",
        "outputId": "d0806543-81e4-4521-d0bb-43f362203129",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train_tokenized=[word_tokenize(i) for i in y_train]\n",
        "y_test_tokenized=[word_tokenize(i) for i in y_test]\n",
        "print(y_train_tokenized[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Kate', 'Winslet', 'to', 'work', 'with', 'Titanic', 'maker', 'after', 'years']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqdSY1PqqeL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = set([])\n",
        "for s in X_train_tokenized:\n",
        "    for w in s:\n",
        "      if len(w)>1:\n",
        "        words.add(w.lower())\n",
        "for s in y_train_tokenized:\n",
        "    for w in s:\n",
        "      if len(w)>1:\n",
        "        words.add(w.lower())\n",
        "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
        "word2index['-PAD-'] = 0  # The special value used for padding\n",
        "word2index['-OOV-'] = 1  # The special value used for OOVs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IqWLGE2v3-h",
        "colab_type": "code",
        "outputId": "3c12edc0-efe5-41c5-b2a5-8d1956c2dffb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-18 17:25:35--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2019-05-18 17:25:35--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1       18%[==>                 ] 152.74M  8.24MB/s    eta 75s    ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E767o2kOv7QC",
        "colab_type": "code",
        "outputId": "da21d6b5-ebad-4e70-b794-4cff46ae4247",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!unzip \"glove.6B.zip\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xl0omApv-UN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "embeddings_index = dict()\n",
        "f = open('glove.6B.300d.txt',encoding=\"utf-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iogkAHIowA3M",
        "colab_type": "code",
        "outputId": "35a9e98b-2ce3-4c79-cabc-4843122f75c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "train_sentences_X, test_sentences_X,train_sentences_Y,test_sentences_Y = [], [], [], []\n",
        "\n",
        "EMB_DIM=300\n",
        "num_words=len(word2index)+1\n",
        "print(\"Number of Words:\"+str(num_words))\n",
        "\n",
        "for s in X_train_tokenized:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "      if len(w)>1:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        "    \n",
        "for s in X_test_tokenized:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "      if len(w)>1:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        "  \n",
        "for s in y_train_tokenized:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "      if len(w)>1:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    train_sentences_Y.append(s_int)\n",
        "    \n",
        "for s in y_test_tokenized:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "       if len(w)>1:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    test_sentences_Y.append(s_int)\n",
        "\n",
        "\n",
        "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
        "print(\"Max Length: \"+str(MAX_LENGTH))  # 271\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "#y_train=to_categorical(y_train)\n",
        "#y_test=to_categorical(y_test)\n",
        "print(train_sentences_X[0])\n",
        "print(train_sentences_Y[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Words:71055\n",
            "Max Length: 68\n",
            "[31734 49094 59050 13417 52178 25965  4519 40091 27194 22857 17123 36915\n",
            " 44410 53024 65963 43275  4519 41033 23617 25350 15356 33723 68529 55323\n",
            " 64066  1123 65210 27194 22857 56145 44410 51544 59050 33050 65334 60021\n",
            " 21375 64694 53027 66328 48125 64066 25685 68030 61547  4410 22857 45730\n",
            " 40197 66328 15501  2226  4957  4519 31474 27194  8317 66328 57828     0\n",
            "     0     0     0     0     0     0     0     0]\n",
            "[59050, 13417, 66328, 51625, 43881, 22857, 56598, 1123, 65210]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "difuNX4awHnQ",
        "colab_type": "code",
        "outputId": "e3c73636-bee3-4306-8b40-2a208e9e6207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embedding_matrix=np.zeros((num_words,EMB_DIM))\n",
        "#print(word2index)\n",
        "numNoEmb=0\n",
        "for word,i in word2index.items():\n",
        "    if i>num_words:\n",
        "        continue\n",
        "    embedding_vector=embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i]=embedding_vector\n",
        "    else:\n",
        "      numNoEmb+=1\n",
        "print(numNoEmb)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfxnS2FnLFEO",
        "colab_type": "code",
        "outputId": "8a33ba26-969f-440e-a4ed-e9477a42c758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential,load_model\n",
        "from tensorflow.keras.layers import Input,Dense, CuDNNLSTM,LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation,Dropout,Add\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.initializers import Constant\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "HIDDEN_UNITS=256\n",
        "\"\"\"\n",
        "Bidirectional LSTM: Others Inspired Encoder-Decoder-seq2seq\n",
        "\"\"\"\n",
        "encoder_inputs = Input(shape=(MAX_LENGTH,))\n",
        "encoder_embedding = Embedding(num_words,EMB_DIM,embeddings_initializer=Constant(embedding_matrix),input_length=MAX_LENGTH,trainable=False,mask_zero=True)(encoder_inputs)\n",
        "encoder_LSTM = LSTM(HIDDEN_UNITS, return_state=True)\n",
        "encoder_LSTM_R = LSTM(HIDDEN_UNITS, return_state=True, go_backwards=True)\n",
        "encoder_outputs_R, state_h_R, state_c_R = encoder_LSTM_R(encoder_embedding)\n",
        "encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
        "\n",
        "final_h = Add()([state_h, state_h_R])\n",
        "final_c = Add()([state_c, state_c_R])\n",
        "encoder_states = [final_h, final_c]\n",
        "\n",
        "\"\"\"\n",
        "decoder\n",
        "\"\"\"\n",
        "decoder_inputs = Input(shape=(MAX_LENGTH,))\n",
        "decoder_embedding = Embedding(num_words,EMB_DIM,embeddings_initializer=Constant(embedding_matrix),input_length=MAX_LENGTH,trainable=False,mask_zero=True)(decoder_inputs)\n",
        "decoder_LSTM = LSTM(HIDDEN_UNITS, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=encoder_states) \n",
        "decoder_dense = Dense(num_words, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model= tf.keras.Model(inputs=[encoder_inputs,decoder_inputs], outputs=decoder_outputs)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.0001),    \n",
        "              metrics=[\"accuracy\"])\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model_seq2seq.png')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 68)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 68, 300)      21316500    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 68)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 256), (None, 570368      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 256), (None, 570368      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 68, 300)      21316500    input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 256)          0           lstm[0][1]                       \n",
            "                                                                 lstm_1[0][1]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 256)          0           lstm[0][2]                       \n",
            "                                                                 lstm_1[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 68, 256), (N 570368      embedding_1[0][0]                \n",
            "                                                                 add[0][0]                        \n",
            "                                                                 add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 68, 71055)    18261135    lstm_2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 62,605,239\n",
            "Trainable params: 19,972,239\n",
            "Non-trainable params: 42,633,000\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmzMab8fTb4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_samples = len(train_sentences_Y)\n",
        "decoder_output_data = np.zeros((num_samples, MAX_LENGTH, num_words), dtype=\"int32\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}