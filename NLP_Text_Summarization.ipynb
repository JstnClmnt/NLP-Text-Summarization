{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Text-Summarization",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JstnClmnt/NLP-Text-Summarization/blob/master/NLP_Text_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mYFMy1_NPGQ",
        "colab_type": "text"
      },
      "source": [
        "# News Article Text Summarization\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH0KS6tFPOx1",
        "colab_type": "text"
      },
      "source": [
        "### Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruSXNIBlSo6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import numpy as np\n",
        "import re\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAOiz_fZPU01",
        "colab_type": "text"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "275RJ-74TJUi",
        "colab_type": "code",
        "outputId": "2a8c171a-1de5-41f8-8c01-443758a13adc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy6vODp4PrYf",
        "colab_type": "text"
      },
      "source": [
        "### Assign to Data Frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJB2GnwJUZJU",
        "colab_type": "code",
        "outputId": "5ab55a18-3797-41b9-8c81-5f7689b866ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"/content/drive/Team Drives/AI Lords/NLP-Text-Summarization/data/news_summary_more.csv\",encoding=\"latin-1\")\n",
        "df.head()\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headlines</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
              "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
              "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
              "      <td>New Zealand defeated India by 8 wickets in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
              "      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
              "      <td>Speaking about the sexual harassment allegatio...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           headlines                                               text\n",
              "0  upGrad learner switches to career in ML & Al w...  Saurav Kant, an alumnus of upGrad and IIIT-B's...\n",
              "1  Delhi techie wins free food from Swiggy for on...  Kunal Shah's credit card bill payment platform...\n",
              "2  New Zealand end Rohit Sharma-led India's 12-ma...  New Zealand defeated India by 8 wickets in the...\n",
              "3  Aegon life iTerm insurance plan helps customer...  With Aegon Life iTerm Insurance plan, customer...\n",
              "4  Have known Hirani for yrs, what if MeToo claim...  Speaking about the sexual harassment allegatio..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUBzG4lWrCW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['headlines'] = df['headlines'].str.replace(\"[.,]\", \" \")\n",
        "df['text'] = df['text'].str.replace(\"[.,]\", \" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2k4lA_Mphv_",
        "colab_type": "code",
        "outputId": "de447514-5eeb-43a6-b58e-54cdccde26ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "df[\"text\"][1]"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Kunal Shah's credit card bill payment platform  CRED  gave users a chance to win free food from Swiggy for one year  Pranav Kaushik  a Delhi techie  bagged this reward after spending 2000 CRED coins  Users get one CRED coin per rupee of bill paid  which can be used to avail rewards from brands like Ixigo  BookMyShow  UberEats  Cult Fit and more \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdIkHeY5pbOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(df[\"text\"], df[\"headlines\"], test_size=0.20, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WawX-jhqtFUb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e5a081b8-ab67-4c85-d6a5-a58c5dd33ca0"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    text = str(text)\n",
        "    text=text.split(\" \")\n",
        "    words=[]\n",
        "    for t in text:\n",
        "      words.append(t)\n",
        "    text=\" \".join(words)\n",
        "    text=re.sub(\"what's\",\"what is \",text)\n",
        "    text=re.sub(\"it's\",\"it is \",text)\n",
        "    text=re.sub(\"\\'ve\",\" have \",text)\n",
        "    text=re.sub(\"i'm\",\"i am \",text)\n",
        "    text=re.sub(\"\\'re\",\" are \",text)\n",
        "    text=re.sub(\"can't\",\" cannot \",text)\n",
        "    text=re.sub(\"n't\",\" not \",text)\n",
        "    text=re.sub(\"\\'d\",\" would \",text)\n",
        "    text=re.sub(\"\\'s\",\" \",text)\n",
        "    text=re.sub(\"\\'\",\" \",text)\n",
        "    text=re.sub(\"\\\"\",\" \",text)\n",
        "    text=re.sub(\"\\'ll\",\" will \",text)\n",
        "    text=re.sub(\" e g \",\" eg \",text)\n",
        "    text=re.sub(\"e-mail\",\"email\",text)\n",
        "    text=re.sub(\"9\\\\/11\",\" 911 \",text)\n",
        "    text=re.sub(\" u.s\",\" american \",text)\n",
        "    text=re.sub(\" u.n\",\" united nations \",text)\n",
        "    text=re.sub(\"\\n\",\" \",text)\n",
        "    text=re.sub(\":\",\" \",text)\n",
        "    text=re.sub(\"\\_\",\" \",text)\n",
        "    text=re.sub(\"\\d+\",\" \",text)\n",
        "    text=re.sub(\"[$#@%&*!~?%{}()]\",\" \",text)\n",
        "    return word_tokenize(text)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxBxffJa5Q3H",
        "colab_type": "code",
        "outputId": "47cf58f5-e6dd-4537-eaad-629a85656c5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "X_train_tokenized=[remove_special_characters(i) for i in X_train]\n",
        "X_test_tokenized=[remove_special_characters(i) for i in X_test]\n",
        "print(X_train_tokenized[0])"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hollywood', 'actress', 'Kate', 'Winslet', 'has', 'joined', 'the', 'cast', 'of', 'Titanic', 'director', 'James', 'Cameron', 'upcoming', 'films', 'in', 'the', 'Avatar', 'franchise', 'making', 'this', 'their', 'first', 'venture', 'together', 'after', 'years', 'of', 'Titanic', 'release', 'Cameron', 'said', 'Kate', 'and', 'I', 'have', 'been', 'looking', 'for', 'something', 'to', 'do', 'together', 'since', 'our', 'collaboration', 'on', 'Titanic', 'I', 'can', 'not', 'wait', 'to', 'see', 'her', 'bring', 'the', 'character', 'of', 'Ronal', 'to', 'life']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZfUX-LmsONj",
        "colab_type": "code",
        "outputId": "d1f116a5-9416-44d8-85d7-938b6ccf52da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train_tokenized=[remove_special_characters(i) for i in y_train]\n",
        "y_test_tokenized=[remove_special_characters(i) for i in y_test]\n",
        "print(y_train_tokenized[0])"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Kate', 'Winslet', 'to', 'work', 'with', 'Titanic', 'maker', 'after', 'years']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqdSY1PqqeL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = set([])\n",
        "for s in X_train_tokenized:\n",
        "    for w in s:\n",
        "        words.add(w.lower())\n",
        "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
        "word2index['-PAD-'] = 0  # The special value used for padding\n",
        "word2index['-OOV-'] = 1  # The special value used for OOVs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IqWLGE2v3-h",
        "colab_type": "code",
        "outputId": "f974eae0-480b-4b7d-c44f-c6fc4f7e984f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-16 05:50:16--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2019-05-16 05:50:16--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  16.1MB/s    in 67s     \n",
            "\n",
            "2019-05-16 05:51:23 (12.3 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E767o2kOv7QC",
        "colab_type": "code",
        "outputId": "afff3628-7012-4ade-b430-e12f6755e0ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "!unzip \"glove.6B.zip\""
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xl0omApv-UN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "embeddings_index = dict()\n",
        "f = open('glove.6B.300d.txt',encoding=\"utf-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iogkAHIowA3M",
        "colab_type": "code",
        "outputId": "3685863a-df36-4865-f8c5-3b6fef85725c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "train_sentences_X, test_sentences_X,train_sentences_Y,test_sentences_Y = [], [], [], []\n",
        "\n",
        "EMB_DIM=300\n",
        "num_words=len(word2index)+1\n",
        "print(\"Number of Words:\"+str(num_words))\n",
        "\n",
        "for s in X_train_tokenized:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        "    \n",
        "for s in X_test_tokenized:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        "  \n",
        "for s in y_train_tokenized:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    train_sentences_Y.append(s_int)\n",
        "    \n",
        "for s in y_test_tokenized:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    test_sentences_Y.append(s_int)\n",
        "\n",
        "\n",
        "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
        "print(\"Max Length: \"+str(MAX_LENGTH))  # 271\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "#y_train=to_categorical(y_train)\n",
        "#y_test=to_categorical(y_test)\n",
        "print(train_sentences_X[0])\n",
        "print(train_sentences_Y[0])"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Words:84640\n",
            "Max Length: 88\n",
            "[69161 22536  9177 41056 79623 35987 54151 67350 12012  4645 15799 38573\n",
            "  2605  5961 56547 83068 54151 74550 48976 51739 39578 26251   831 27708\n",
            " 71969 69489 77000 12012  4645 41791  2605 34393  9177 70676 10976 81506\n",
            " 58964 26797 15986  3390 70630 60261 71969 28796 73170 63307 83989  4645\n",
            " 10976  7455 48038  6186 70630 36878 46073 71439 54151 62929 12012 46026\n",
            " 70630 45654     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0]\n",
            "[9177, 41056, 70630, 12997, 80457, 4645, 47262, 69489, 77000]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "difuNX4awHnQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix=np.zeros((num_words,EMB_DIM))\n",
        "#print(word2index)\n",
        "for word,i in word2index.items():\n",
        "    if i>num_words:\n",
        "        continue\n",
        "    embedding_vector=embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i]=embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}